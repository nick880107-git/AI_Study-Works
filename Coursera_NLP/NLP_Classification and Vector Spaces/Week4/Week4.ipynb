{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlz9ewWMFslZ"
      },
      "source": [
        "# Naive Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccm7oaZsFfKY"
      },
      "source": [
        "import pdb\r\n",
        "import pickle\r\n",
        "import string\r\n",
        "\r\n",
        "import time\r\n",
        "\r\n",
        "import gensim\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "import numpy as np\r\n",
        "import scipy\r\n",
        "import sklearn\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from nltk.corpus import stopwords, twitter_samples\r\n",
        "from nltk.tokenize import TweetTokenizer\r\n",
        "\r\n",
        "from utils import (cosine_similarity, get_dict, process_tweet)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Qn1-QNIkhQ",
        "outputId": "954b1a54-953d-49fb-89b3-0ea79db6007b"
      },
      "source": [
        "nltk.download('stopwords')\r\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw7bQB58LgMg"
      },
      "source": [
        "### 下載所需資料並製作資料集\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-00YfRelKRtn"
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\r\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLkZNCd1LcnY",
        "outputId": "8e37a133-f065-45ee-9bc4-6097e65124f7"
      },
      "source": [
        "!curl -o ./wiki.multi.fr.vec https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  599M  100  599M    0     0  11.3M      0  0:00:52  0:00:52 --:--:-- 11.9M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU8DF2XTJDe_",
        "outputId": "d4130802-66fa-45bc-ac87-6cbc709d6007"
      },
      "source": [
        "# 使用KeyedVectors製作word vector\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "en_embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\r\n",
        "fr_embeddings = KeyedVectors.load_word2vec_format('./wiki.multi.fr.vec')\r\n",
        "\r\n",
        "\r\n",
        "# 讀取我們的訓練集 & 測試集(.txt)\r\n",
        "# 使用util中的 get_dict()，他會把傳入的檔案依照行列製作成dict並回傳\r\n",
        "en_fr_train = get_dict('en-fr.train.txt')\r\n",
        "print('The length of the english to french training dictionary is', len(en_fr_train))\r\n",
        "en_fr_test = get_dict('en-fr.test.txt')\r\n",
        "print('The length of the english to french test dictionary is', len(en_fr_train))\r\n",
        "\r\n",
        "english_set = set(en_embeddings.vocab)\r\n",
        "french_set = set(fr_embeddings.vocab)\r\n",
        "en_embeddings_subset = {}\r\n",
        "fr_embeddings_subset = {}\r\n",
        "french_words = set(en_fr_train.values())\r\n",
        "\r\n",
        "# 針對訓練集&測試集的單字，從en/fr單字語料庫中取出子集合\r\n",
        "for en_word in en_fr_train.keys():\r\n",
        "    fr_word = en_fr_train[en_word]\r\n",
        "    if fr_word in french_set and en_word in english_set:\r\n",
        "        en_embeddings_subset[en_word] = en_embeddings[en_word]\r\n",
        "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\r\n",
        "\r\n",
        "\r\n",
        "for en_word in en_fr_test.keys():\r\n",
        "    fr_word = en_fr_test[en_word]\r\n",
        "    if fr_word in french_set and en_word in english_set:\r\n",
        "        en_embeddings_subset[en_word] = en_embeddings[en_word]\r\n",
        "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\r\n",
        "\r\n",
        "# 存成pickle格式，下次便可透過load直接使用\r\n",
        "pickle.dump( en_embeddings_subset, open( \"en_embeddings.p\", \"wb\" ) )\r\n",
        "pickle.dump( fr_embeddings_subset, open( \"fr_embeddings.p\", \"wb\" ) )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of the english to french training dictionary is 5000\n",
            "The length of the english to french test dictionary is 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyWyyErmRP-P"
      },
      "source": [
        "### 若之前已經處理過資料集，便可從此處讀取"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_MYt3DeQ1SQ"
      },
      "source": [
        "en_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\r\n",
        "fr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))\r\n",
        "en_fr_train = get_dict('en-fr.train.txt')\r\n",
        "print('The length of the english to french training dictionary is', len(en_fr_train))\r\n",
        "en_fr_test = get_dict('en-fr.test.txt')\r\n",
        "print('The length of the english to french test dictionary is', len(en_fr_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qokM_40KWL8_"
      },
      "source": [
        "### 訓練一matrix R 盡可能滿足XR = Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7I1zUoGRnOF"
      },
      "source": [
        "def get_matrices(en_fr, french_vecs, english_vecs):\r\n",
        "    \"\"\"\r\n",
        "    Input:\r\n",
        "        en_fr: English to French dictionary\r\n",
        "        french_vecs: French words to their corresponding word embeddings.\r\n",
        "        english_vecs: English words to their corresponding word embeddings.\r\n",
        "    Output: \r\n",
        "        X: a matrix where the columns are the English embeddings.\r\n",
        "        Y: a matrix where the columns correspong to the French embeddings.\r\n",
        "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "\r\n",
        "    # X_l and Y_l are lists of the english and french word embeddings\r\n",
        "    X_l = list()\r\n",
        "    Y_l = list()\r\n",
        "\r\n",
        "    # 取出english_vecs的keys\r\n",
        "    english_set = english_vecs.keys()\r\n",
        "\r\n",
        "    # 取出french_vecs的keys\r\n",
        "    french_set = french_vecs.keys()\r\n",
        "\r\n",
        "    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\r\n",
        "    french_words = set(en_fr.values())\r\n",
        "\r\n",
        "    # 針對英法dict中的內容，分別取出英文詞在english_vecs的word embeddings，以及法文詞在french_vecs中的word embeddings\r\n",
        "    # 並存在X_l及Y_l中\r\n",
        "    for en_word, fr_word in en_fr.items():\r\n",
        "\r\n",
        "        # check that the french word has an embedding and that the english word has an embedding\r\n",
        "        if fr_word in french_set and en_word in english_set:\r\n",
        "\r\n",
        "            # get the english embedding\r\n",
        "            en_vec = english_vecs[en_word]\r\n",
        "\r\n",
        "            # get the french embedding\r\n",
        "            fr_vec = french_vecs[fr_word]\r\n",
        "\r\n",
        "            # add the english embedding to the list\r\n",
        "            X_l.append(en_vec)\r\n",
        "\r\n",
        "            # add the french embedding to the list\r\n",
        "            Y_l.append(fr_vec)\r\n",
        "\r\n",
        "    # 把list疊成matrix\r\n",
        "    X = np.vstack(X_l)\r\n",
        "    Y = np.vstack(Y_l)\r\n",
        "    ### END CODE HERE ###\r\n",
        "\r\n",
        "    return X, Y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrnzvD20Tz3A"
      },
      "source": [
        "X_train, Y_train = get_matrices(en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js7TWff3UeUf"
      },
      "source": [
        "def compute_loss(X, Y, R):\r\n",
        "    '''\r\n",
        "    Inputs: \r\n",
        "        X: 英文詞嵌入的matrix(m*n).\r\n",
        "        Y: 法文詞嵌入的matrix(m*n).\r\n",
        "        R: 英→法文詞嵌入的matrix(n*n).\r\n",
        "    Outputs:\r\n",
        "        L: Loss(XR-Y).\r\n",
        "    '''\r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "\r\n",
        "    m = X.shape[0]\r\n",
        "    diff = np.dot(X,R)-Y\r\n",
        "    diff_squared = diff**2\r\n",
        "    sum_diff_squared = np.sum(diff_squared)\r\n",
        "    loss = sum_diff_squared/m\r\n",
        "    \r\n",
        "    ### END CODE HERE ###\r\n",
        "    return loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzVwmrF5VSK_"
      },
      "source": [
        "def compute_gradient(X, Y, R):\r\n",
        "    '''\r\n",
        "    Inputs: \r\n",
        "        X: 英文詞嵌入的matrix(m*n).\r\n",
        "        Y: 法文詞嵌入的matrix(m*n).\r\n",
        "        R: 英→法文詞嵌入的matrix(n*n).\r\n",
        "    Outputs:\r\n",
        "        g: gradient loss function L for given X, Y and R.\r\n",
        "    '''\r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    # m is the number of rows in X\r\n",
        "    m = X.shape[0]\r\n",
        "\r\n",
        "    # gradient is X^T(XR - Y) * 2/m\r\n",
        "    gradient = np.dot(X.transpose(),np.dot(X,R)-Y)*(2/m)\r\n",
        "    ### END CODE HERE ###\r\n",
        "    return gradient"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx9F18GZVm-w"
      },
      "source": [
        "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\r\n",
        "    '''\r\n",
        "    Inputs:\r\n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\r\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\r\n",
        "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\r\n",
        "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\r\n",
        "    Outputs:\r\n",
        "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\r\n",
        "    '''\r\n",
        "    np.random.seed(129)\r\n",
        "\r\n",
        "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\r\n",
        "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\r\n",
        "    R = np.random.rand(X.shape[1], X.shape[1])\r\n",
        "\r\n",
        "    for i in range(train_steps):\r\n",
        "        if i % 25 == 0:\r\n",
        "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\r\n",
        "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "        # use the function that you defined to compute the gradient\r\n",
        "        gradient = compute_gradient(X,Y,R)\r\n",
        "\r\n",
        "        # update R by subtracting the learning rate times gradient\r\n",
        "        R -=  learning_rate * gradient\r\n",
        "        ### END CODE HERE ###\r\n",
        "    return R"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQHNKY19V79_",
        "outputId": "7d2a6d9b-6865-44dc-b8ae-91246dfce73b"
      },
      "source": [
        "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss at iteration 0 is: 963.0146\n",
            "loss at iteration 25 is: 97.8292\n",
            "loss at iteration 50 is: 26.8329\n",
            "loss at iteration 75 is: 9.7893\n",
            "loss at iteration 100 is: 4.3776\n",
            "loss at iteration 125 is: 2.3281\n",
            "loss at iteration 150 is: 1.4480\n",
            "loss at iteration 175 is: 1.0338\n",
            "loss at iteration 200 is: 0.8251\n",
            "loss at iteration 225 is: 0.7145\n",
            "loss at iteration 250 is: 0.6534\n",
            "loss at iteration 275 is: 0.6185\n",
            "loss at iteration 300 is: 0.5981\n",
            "loss at iteration 325 is: 0.5858\n",
            "loss at iteration 350 is: 0.5782\n",
            "loss at iteration 375 is: 0.5735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB0eW03rW0ZI"
      },
      "source": [
        "### 利用KNN找尋XR預測出的Y' 較接近哪個Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzfno6svW4gq"
      },
      "source": [
        "def nearest_neighbor(v, candidates, k=1):\r\n",
        "    \"\"\"\r\n",
        "    Input:\r\n",
        "      - v, the vector you are going find the nearest neighbor for\r\n",
        "      - candidates: a set of vectors where we will find the neighbors\r\n",
        "      - k: top k nearest neighbors to find\r\n",
        "    Output:\r\n",
        "      - k_idx: the indices of the top k closest vectors in sorted form\r\n",
        "    \"\"\"\r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    similarity_l = []\r\n",
        "\r\n",
        "    # for each candidate vector...\r\n",
        "    for row in candidates:\r\n",
        "        # get the cosine similarity\r\n",
        "        cos_similarity = cosine_similarity(v,row)\r\n",
        "\r\n",
        "        # append the similarity to the list\r\n",
        "        similarity_l.append(cos_similarity)\r\n",
        "        \r\n",
        "    # 排序(由大到小)\r\n",
        "    sorted_ids = np.argsort(similarity_l)\r\n",
        "    k_idx = sorted_ids[-k:]\r\n",
        "    ### END CODE HERE ###\r\n",
        "    return k_idx"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19b9cwudXkBw"
      },
      "source": [
        "def test_vocabulary(X, Y, R):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        X: a matrix where the columns are the English embeddings.\r\n",
        "        Y: a matrix where the columns correspong to the French embeddings.\r\n",
        "        R: the transform matrix which translates word embeddings from\r\n",
        "        English to French word vector space.\r\n",
        "    Output:\r\n",
        "        accuracy: for the English to French capitals\r\n",
        "    '''\r\n",
        "\r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    # The prediction is X times R\r\n",
        "    pred = np.dot(X,R)\r\n",
        "\r\n",
        "    # initialize the number correct to zero\r\n",
        "    num_correct = 0\r\n",
        "\r\n",
        "    # loop through each row in pred (each transformed embedding)\r\n",
        "    for i in range(len(pred)):\r\n",
        "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\r\n",
        "        pred_idx = nearest_neighbor(pred[i],Y)\r\n",
        "\r\n",
        "        # if the index of the nearest neighbor equals the row of i... \\\r\n",
        "        if pred_idx == i:\r\n",
        "            # increment the number correct by 1.\r\n",
        "            num_correct += 1\r\n",
        "\r\n",
        "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\r\n",
        "    accuracy = num_correct / len(pred)\r\n",
        "\r\n",
        "    ### END CODE HERE ###\r\n",
        "\r\n",
        "    return accuracy"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkdlaLSlZMg3",
        "outputId": "1ec0c7cb-3006-49a2-c86e-edef5758756e"
      },
      "source": [
        "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)\r\n",
        "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\r\n",
        "print(f\"accuracy on test set is {acc:.3f}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy on test set is 0.557\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}