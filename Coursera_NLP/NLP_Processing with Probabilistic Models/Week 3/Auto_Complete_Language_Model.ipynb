{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auto-Complete Language Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd_r0qeC90a1"
      },
      "source": [
        "\r\n",
        "\r\n",
        "1. 讀取/預處理資料\r\n",
        "  *   讀取你的語料庫(corpus)並斷詞(tokenize)處理\r\n",
        "  *   切分訓練/測試集\r\n",
        "  *   對出現頻率低的詞統一標記為<unk>\r\n",
        "\r\n",
        "2. 開發你的N-gram語言模型\r\n",
        "  *   計算給定資料集的n-gram\r\n",
        "  *   使用k-smoothing計算下一個字的條件機率\r\n",
        "3. 評估你的N-gram模型\r\n",
        "4. 實際測試\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IFaeBXJ9btV",
        "outputId": "bb018842-21f4-4b7f-fa7f-c98a1a69317d"
      },
      "source": [
        "import math\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJfnXF5K_RoH"
      },
      "source": [
        "# 斷句\r\n",
        "def split_sentences(data):\r\n",
        "  sentences = data.split('\\n')  # 依換行符號切分字串\r\n",
        "  sentences = [s.strip() for s in sentences]  # .strip()去除字串前後無意義的字元，若指定參數則去除特定字元\r\n",
        "  sentences = [s for s in sentences if len(s) > 0]  # 去除空白句子\r\n",
        "  return sentences"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtFZZ-TOAGXg",
        "outputId": "87c925f9-170b-4cf5-8591-ec6441f163a9"
      },
      "source": [
        "x = \"\"\"\r\n",
        "I have a pen.\r\n",
        "Ihave an apple.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "split_sentences(x)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have a pen.', 'Ihave an apple.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tyoRaSyAPJ3"
      },
      "source": [
        "# 斷詞\r\n",
        "def tokenize_sentences(sentences):\r\n",
        "  tokenize_sentences = []\r\n",
        "  for s in sentences:\r\n",
        "    s = s.lower()\r\n",
        "    token = nltk.word_tokenize(s)\r\n",
        "    tokenize_sentences.append(token)\r\n",
        "\r\n",
        "  \r\n",
        "  return tokenize_sentences"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbUK57m9ChgI",
        "outputId": "88c9a1f4-8786-4c07-de9e-8d233259fcae"
      },
      "source": [
        "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\r\n",
        "tokenize_sentences(sentences)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['sky', 'is', 'blue', '.'],\n",
              " ['leaves', 'are', 'green', '.'],\n",
              " ['roses', 'are', 'red', '.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDNufOKdC7K1"
      },
      "source": [
        "# 資料預處理-1\r\n",
        "def get_tokenized_data(data):\r\n",
        "  sentences = split_sentences(data)\r\n",
        "  tokenized_sentences = tokenize_sentences(sentences)\r\n",
        "\r\n",
        "  return tokenized_sentences"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z0ouiT5DTyV"
      },
      "source": [
        "# 讀取語料庫\r\n",
        "with open(\"en_US.twitter.txt\", \"r\") as f:\r\n",
        "    data = f.read()\r\n",
        "tokenized_data = get_tokenized_data(data)\r\n",
        "random.seed(87)\r\n",
        "random.shuffle(tokenized_data)\r\n",
        "t = int(len(tokenized_data) * 0.8)\r\n",
        "train_data = tokenized_data[0:t]\r\n",
        "test_data = tokenized_data[t:] "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azsMr9iBG-CB",
        "outputId": "30c4ac1a-755f-4142-9c97-ae57b72353cb"
      },
      "source": [
        "print(\"{} data are split into {} train and {} test set\".format(\r\n",
        "    len(tokenized_data), len(train_data), len(test_data)))\r\n",
        "\r\n",
        "print(\"First training sample:\")\r\n",
        "print(train_data[0])\r\n",
        "      \r\n",
        "print(\"First test sample\")\r\n",
        "print(test_data[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47961 data are split into 38368 train and 9593 test set\n",
            "First training sample:\n",
            "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n",
            "First test sample\n",
            "['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGKMVUrEIgH5"
      },
      "source": [
        "# 計算詞頻\r\n",
        "def count_words(tokenized_sentences):\r\n",
        "  word_counts = {}\r\n",
        "  for sentence in tokenized_sentences:\r\n",
        "    for token in sentence:\r\n",
        "      if token not in word_counts.keys():\r\n",
        "        word_counts[token] = 1\r\n",
        "      else:\r\n",
        "        word_counts[token] += 1\r\n",
        "  return word_counts"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTdvnaWXJRdE",
        "outputId": "1d57417a-2fda-4541-9808-4e42c6b19074"
      },
      "source": [
        "tokenized_sentences = [['sky', 'is', 'blue', '.'],\r\n",
        "                       ['leaves', 'are', 'green', '.'],\r\n",
        "                       ['roses', 'are', 'red', '.']]\r\n",
        "count_words(tokenized_sentences)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 3,\n",
              " 'are': 2,\n",
              " 'blue': 1,\n",
              " 'green': 1,\n",
              " 'is': 1,\n",
              " 'leaves': 1,\n",
              " 'red': 1,\n",
              " 'roses': 1,\n",
              " 'sky': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G6iku8WJh57"
      },
      "source": [
        "# 處理closed vocabulary，只計算詞頻達一定程度的字\r\n",
        "def get_words_with_nplus_frequency(tokenized_sentences, frequency):\r\n",
        "  closed_vocab = []\r\n",
        "  word_counts = count_words(tokenized_sentences)\r\n",
        "  for word, count in word_counts.items():\r\n",
        "    if count >= frequency:\r\n",
        "      closed_vocab.append(word)\r\n",
        "  return closed_vocab"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGhvrHsbKcqA",
        "outputId": "8feccfab-657b-4fee-c9c3-5282506c3f94"
      },
      "source": [
        "tokenized_sentences = [['sky', 'is', 'blue', '.'],\r\n",
        "                       ['leaves', 'are', 'green', '.'],\r\n",
        "                       ['roses', 'are', 'red', '.']]\r\n",
        "tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, frequency=2)\r\n",
        "print(f\"Closed vocabulary:\")\r\n",
        "print(tmp_closed_vocab)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Closed vocabulary:\n",
            "['.', 'are']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL0viogQKgMW"
      },
      "source": [
        "# 處理OOV(out of vocabulary)，詞頻過低的字全用unk取代\r\n",
        "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\r\n",
        "  vocabulary = set(vocabulary)\r\n",
        "  replaced_tokenized_sentences = []\r\n",
        "\r\n",
        "  for sentence in tokenized_sentences:\r\n",
        "    replaced_sentence = []\r\n",
        "    for token in sentence:\r\n",
        "      if token in vocabulary:\r\n",
        "        replaced_sentence.append(token)\r\n",
        "      else:\r\n",
        "        replaced_sentence.append(unknown_token)\r\n",
        "    replaced_tokenized_sentences.append(replaced_sentence)\r\n",
        "  return replaced_tokenized_sentences"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89hyCwatLyek",
        "outputId": "044e0f98-f01d-4a8e-c1fe-4187a2853cb5"
      },
      "source": [
        "tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\r\n",
        "vocabulary = [\"dogs\", \"sleep\"]\r\n",
        "tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\r\n",
        "print(f\"Original sentence:\")\r\n",
        "print(tokenized_sentences)\r\n",
        "print(f\"tokenized_sentences with less frequent words converted to '<unk>':\")\r\n",
        "print(tmp_replaced_tokenized_sentences)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sentence:\n",
            "[['dogs', 'run'], ['cats', 'sleep']]\n",
            "tokenized_sentences with less frequent words converted to '<unk>':\n",
            "[['dogs', '<unk>'], ['<unk>', 'sleep']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKMyKbpwMKRt"
      },
      "source": [
        "# 資料預處理-2\r\n",
        "def preprocess_data(train_data,test_data,frequency):\r\n",
        "  vocabulary = get_words_with_nplus_frequency(train_data,frequency)\r\n",
        "  train_data_replaced = replace_oov_words_by_unk(train_data,vocabulary)\r\n",
        "  test_data_replaced = replace_oov_words_by_unk(test_data,vocabulary)\r\n",
        "\r\n",
        "  return train_data_replaced, test_data_replaced, vocabulary"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48zri4VBM11d"
      },
      "source": [
        "frequency = 2\r\n",
        "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data,test_data,frequency)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp7vrvWfNEHE",
        "outputId": "e68fecee-b39a-4f6a-9439-b9b684210562"
      },
      "source": [
        "print(\"First preprocessed training sample:\")\r\n",
        "print(train_data_processed[0])\r\n",
        "print()\r\n",
        "print(\"First preprocessed test sample:\")\r\n",
        "print(test_data_processed[0])\r\n",
        "print()\r\n",
        "print(\"First 10 vocabulary:\")\r\n",
        "print(vocabulary[0:10])\r\n",
        "print()\r\n",
        "print(\"Size of vocabulary:\", len(vocabulary))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First preprocessed training sample:\n",
            "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n",
            "\n",
            "First preprocessed test sample:\n",
            "['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>']\n",
            "\n",
            "First 10 vocabulary:\n",
            "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the']\n",
            "\n",
            "Size of vocabulary: 14944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOAFxiqIOGi0"
      },
      "source": [
        "def count_n_grams(data,n,start_token='<s>',end_token = '<e>'):\r\n",
        "  \"\"\"\r\n",
        "    data:被你切成一個個token words的字串的語料庫\r\n",
        "    n:看你要做uni,bi,還是trigrams，在字串前加上等量的<s>\r\n",
        "  \"\"\"\r\n",
        "  n_grams = {}\r\n",
        "  for sentence in data:\r\n",
        "    sentence = [start_token]*n + sentence + [end_token]\r\n",
        "    # list轉tuple，這樣每個字串中的字詞才能作為key\r\n",
        "    sentence = tuple(sentence)\r\n",
        "\r\n",
        "    m = len(sentence) if n ==1 else len(sentence)-1\r\n",
        "    for i in range(m):\r\n",
        "      n_gram = sentence[i:i+n]\r\n",
        "      if n_gram in n_grams.keys():\r\n",
        "        n_grams[n_gram] += 1\r\n",
        "      else:\r\n",
        "        n_grams[n_gram] = 1\r\n",
        "  return n_grams"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_ItnZftQXrP",
        "outputId": "cfbadd55-7e9e-453b-ca12-6dc069a3e613"
      },
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\r\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\r\n",
        "print(\"Uni-gram:\")\r\n",
        "print(count_n_grams(sentences, 1))\r\n",
        "print(\"Bi-gram:\")\r\n",
        "print(count_n_grams(sentences, 2))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uni-gram:\n",
            "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
            "Bi-gram:\n",
            "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blgfBWrYQcQ9"
      },
      "source": [
        "# 計算出現前n個詞，下一個詞是word的機率\r\n",
        "def estimate_probability(word,previous_n_gram,n_gram_counts,n_plus1_gram_counts,vocabulary_size,k=1.0):\r\n",
        "\r\n",
        "  previous_n_gram = tuple(previous_n_gram)\r\n",
        "  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts  else 0\r\n",
        "  denominator = previous_n_gram_count + k * vocabulary_size\r\n",
        "  n_plus1_gram = previous_n_gram + (word,)\r\n",
        "  n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts  else 0\r\n",
        "  numerator = n_plus1_gram_count + k\r\n",
        "  probability = numerator / denominator\r\n",
        "  return probability"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P0-CzFnT6M-"
      },
      "source": [
        "# 將vocabulary中所有的word逐一帶入，計算出現前n個詞，下一個詞是word的機率\r\n",
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\r\n",
        "    \r\n",
        "    previous_n_gram = tuple(previous_n_gram)\r\n",
        "    \r\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\r\n",
        "    vocabulary_size = len(vocabulary)\r\n",
        "    \r\n",
        "    probabilities = {}\r\n",
        "    for word in vocabulary:\r\n",
        "        probability = estimate_probability(word, previous_n_gram,n_gram_counts, n_plus1_gram_counts,vocabulary_size, k=k)\r\n",
        "        probabilities[word] = probability\r\n",
        "\r\n",
        "    return probabilities"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE0D4az1Uj2d"
      },
      "source": [
        "def make_count_matrix(n_plus1_gram_counts, vocabulary):\r\n",
        "\r\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\r\n",
        "    n_grams = []\r\n",
        "    for n_plus1_gram in n_plus1_gram_counts.keys():\r\n",
        "        n_gram = n_plus1_gram[0:-1]\r\n",
        "        n_grams.append(n_gram)\r\n",
        "    n_grams = list(set(n_grams))\r\n",
        "    \r\n",
        "    # mapping from n-gram to row\r\n",
        "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\r\n",
        "    # mapping from next word to column\r\n",
        "    col_index = {word:j for j, word in enumerate(vocabulary)}\r\n",
        "    \r\n",
        "    nrow = len(n_grams)\r\n",
        "    ncol = len(vocabulary)\r\n",
        "    count_matrix = np.zeros((nrow, ncol))\r\n",
        "    for n_plus1_gram, count in n_plus1_gram_counts.items():\r\n",
        "        n_gram = n_plus1_gram[0:-1]\r\n",
        "        word = n_plus1_gram[-1]\r\n",
        "        if word not in vocabulary:\r\n",
        "            continue\r\n",
        "        i = row_index[n_gram]\r\n",
        "        j = col_index[word]\r\n",
        "        count_matrix[i, j] = count\r\n",
        "    \r\n",
        "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\r\n",
        "    return count_matrix"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvJUa9YwWhbj"
      },
      "source": [
        "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\r\n",
        "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\r\n",
        "    count_matrix += k\r\n",
        "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\r\n",
        "    return prob_matrix"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrNpS2qZW6Qt"
      },
      "source": [
        "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\r\n",
        "    \"\"\"\r\n",
        "    Calculate perplexity for a list of sentences\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        sentence: List of strings\r\n",
        "        n_gram_counts: Dictionary of counts of (n+1)-grams\r\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\r\n",
        "        vocabulary_size: number of unique words in the vocabulary\r\n",
        "        k: Positive smoothing constant\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "        Perplexity score\r\n",
        "    \"\"\"\r\n",
        "    # length of previous words\r\n",
        "    n = len(list(n_gram_counts.keys())[0]) \r\n",
        "    \r\n",
        "    # prepend <s> and append <e>\r\n",
        "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\r\n",
        "    \r\n",
        "    # Cast the sentence from a list to a tuple\r\n",
        "    sentence = tuple(sentence)\r\n",
        "    \r\n",
        "    # length of sentence (after adding <s> and <e> tokens)\r\n",
        "    N = len(sentence)\r\n",
        "    \r\n",
        "    # The variable p will hold the product\r\n",
        "    # that is calculated inside the n-root\r\n",
        "    # Update this in the code below\r\n",
        "    product_pi = 1.0\r\n",
        "    \r\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\r\n",
        "    # Index t ranges from n to N - 1\r\n",
        "    for t in range(n, N): # complete this line\r\n",
        "\r\n",
        "        # get the n-gram preceding the word at position t\r\n",
        "        n_gram = sentence[t-n:t]\r\n",
        "        \r\n",
        "        # get the word at position t\r\n",
        "        word = sentence[t]\r\n",
        "        \r\n",
        "        # Estimate the probability of the word given the n-gram\r\n",
        "        # using the n-gram counts, n-plus1-gram counts,\r\n",
        "        # vocabulary size, and smoothing constant\r\n",
        "        probability = estimate_probability(word,n_gram, n_gram_counts, n_plus1_gram_counts, len(unique_words), k=1)\r\n",
        "        \r\n",
        "        # Update the product of the probabilities\r\n",
        "        # This 'product_pi' is a cumulative product \r\n",
        "        # of the (1/P) factors that are calculated in the loop\r\n",
        "        product_pi *= 1 / probability\r\n",
        "\r\n",
        "    # Take the Nth root of the product\r\n",
        "    perplexity = product_pi**(1/float(N))\r\n",
        "    \r\n",
        "    ### END CODE HERE ### \r\n",
        "    return perplexity"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYx0NaOCXBiU"
      },
      "source": [
        "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "    n = len(list(n_gram_counts.keys())[0]) \r\n",
        "    previous_n_gram = previous_tokens[-n:]\r\n",
        "    probabilities = estimate_probabilities(previous_n_gram,n_gram_counts, n_plus1_gram_counts,vocabulary, k=k)\r\n",
        "    \r\n",
        "    # Initialize suggested word to None\r\n",
        "    # This will be set to the word with highest probability\r\n",
        "    suggestion = None\r\n",
        "    max_prob = 0\r\n",
        "    \r\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\r\n",
        "    \r\n",
        "    # For each word and its probability in the probabilities dictionary:\r\n",
        "    for word, prob in probabilities.items(): # complete this line\r\n",
        "        \r\n",
        "        # If the optional start_with string is set\r\n",
        "        if start_with != None: # complete this line\r\n",
        "\r\n",
        "            # Check if the word starts with the letters in 'start_with'\r\n",
        "            if not word.startswith(start_with): # complete this line\r\n",
        "\r\n",
        "                #If so, don't consider this word (move onto the next word)\r\n",
        "                continue # complete this line\r\n",
        "        \r\n",
        "        # Check if this word's probability\r\n",
        "        # is greater than the current maximum probability\r\n",
        "        if prob > max_prob: # complete this line\r\n",
        "            \r\n",
        "            # If so, save this word as the best suggestion (so far)\r\n",
        "            suggestion = word\r\n",
        "            \r\n",
        "            # Save the new maximum probability\r\n",
        "            max_prob = prob\r\n",
        "\r\n",
        "    ### END CODE HERE\r\n",
        "    \r\n",
        "    return suggestion, max_prob"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duN8q9w5Xizc"
      },
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\r\n",
        "    model_counts = len(n_gram_counts_list)\r\n",
        "    suggestions = []\r\n",
        "    for i in range(model_counts-1):\r\n",
        "        n_gram_counts = n_gram_counts_list[i]\r\n",
        "        n_plus1_gram_counts = n_gram_counts_list[i+1]\r\n",
        "        \r\n",
        "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\r\n",
        "                                    n_plus1_gram_counts, vocabulary,\r\n",
        "                                    k=k, start_with=start_with)\r\n",
        "        suggestions.append(suggestion)\r\n",
        "    return suggestions"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}