{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hXR5WmlBHc0"
      },
      "source": [
        "from utils_pos import get_word_tag, preprocess  #輔助套件，分別定義tag(n、v、a、ad)及observable，以及對語料進行前處理\r\n",
        "import pandas as pd\r\n",
        "from collections import defaultdict #一種dict型別，不過在調用字典中不存在的key時會回傳一個default值而非KeyError\r\n",
        "import math\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPtiqhutC_D3"
      },
      "source": [
        "# 我們使用Wal Street Journal作為我們的資料集\r\n",
        "# WSJ-2_21.pos為training data；WSJ-24.pos為testing data\r\n",
        "\r\n",
        "with open(\"WSJ_02-21.pos\", 'r') as f:\r\n",
        "    training_corpus = f.readlines()\r\n",
        "\r\n",
        "with open(\"WSJ_24.pos\", 'r') as f:\r\n",
        "    y = f.readlines()\r\n",
        "\r\n",
        "# 讀取vocabulary，先依換行符號切割所有words並儲存成list，再用迴圈賦予這些words對應的index(dict)\r\n",
        "with open(\"hmm_vocab.txt\", 'r') as f:\r\n",
        "    voc_l = f.read().split('\\n')\r\n",
        "vocab = {}  \r\n",
        "for i, word in enumerate(sorted(voc_l)): \r\n",
        "    vocab[word] = i      \r\n",
        "\r\n",
        "# 預處理test.words(只有words，沒有tag)，切分裡面所有的words並將vocab以外的字設為\"--unk_(tag類型)---\"\r\n",
        "_, prep = preprocess(vocab, \"test.words\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EQFlXRTe7ZZ"
      },
      "source": [
        "# 製作以下三種dictionary：\r\n",
        "# transition counts：計算從當前tag變成下一tag的個數(如當前是verb，下一個是noun的組合有多少)，用來計算馬可夫模型state間轉換的機率\r\n",
        "# emission counts：計算當前tag下，會是某一word的個數，用來計算隱藏馬可夫模型各state對應到不同observable words的機率\r\n",
        "# tag counts：計算不同tag的出現次數\r\n",
        "\r\n",
        "def create_dictionaries(training_corpus, vocab):\r\n",
        "    \"\"\"\r\n",
        "    Input: \r\n",
        "        training_corpus: 訓練集(word - tag).\r\n",
        "        vocab: vocaburary(word:index)\r\n",
        "    Output: \r\n",
        "        emission_counts: 計算從當前tag變成下一tag的個數(dict)\r\n",
        "        transition_counts: 計算當前tag下，會是某一word的個數(dict)\r\n",
        "        tag_counts:計算不同tag的出現次數(dict)\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # 初始化字典\r\n",
        "    emission_counts = defaultdict(int)\r\n",
        "    transition_counts = defaultdict(int)\r\n",
        "    tag_counts = defaultdict(int)\r\n",
        "    \r\n",
        "    # 初始化開始狀態，以--s--表示\r\n",
        "    prev_tag = '--s--' \r\n",
        "    \r\n",
        "    # 計數用\r\n",
        "    i = 0 \r\n",
        "    \r\n",
        "    # word_tag為training data中的每一行資料(皆為word tag形式)\r\n",
        "    for word_tag in training_corpus:\r\n",
        "\r\n",
        "      i += 1\r\n",
        "      if i % 50000 == 0:\r\n",
        "        print(f\"word count = {i}\")\r\n",
        "            \r\n",
        "      ### START CODE HERE (Replace instances of 'None' with your code) ###\r\n",
        "      # 使用helper中的function，切分word 及 tag並回傳(其中word僅會回傳vocab中有的單字，若無則回傳 \"--unk_(tag類型)--\" )\r\n",
        "      word, tag = get_word_tag(word_tag,vocab) \r\n",
        "        \r\n",
        "      # 依據各字典定義，更新value\r\n",
        "      transition_counts[(prev_tag, tag)] += 1\r\n",
        "      emission_counts[(tag, word)] += 1\r\n",
        "      tag_counts[tag] += 1\r\n",
        "\r\n",
        "      # 儲存當前的tag，用以比較之後tag狀態的轉換\r\n",
        "      prev_tag = tag\r\n",
        "        \r\n",
        "      ### END CODE HERE ###\r\n",
        "        \r\n",
        "    return emission_counts, transition_counts, tag_counts\r\n",
        "\r\n",
        "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)\r\n",
        "states = sorted(tag_counts.keys())  #用list儲存所有tag(後面計算accuracy時用)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApATjPVki4uh"
      },
      "source": [
        "# 僅使用emission計算詞性預測的準確度\r\n",
        "def predict_pos(prep, y, emission_counts, vocab, states):\r\n",
        "    '''\r\n",
        "    Input: \r\n",
        "        prep: 事先處理，沒有tag的y\r\n",
        "        y: testing data(word, tag)\r\n",
        "        emission_counts: 當前tag變成下一tag的個數(dict)\r\n",
        "        vocab: vocaburary(word:index)\r\n",
        "        states: 所有tag類型(list)\r\n",
        "    Output: \r\n",
        "        accuracy: 準確度\r\n",
        "    '''\r\n",
        "    \r\n",
        "    # 辨識正確次數\r\n",
        "    num_correct = 0\r\n",
        "    \r\n",
        "    # 取出(tag,word)並以集合形式儲存(相當於emission matrix中的所有元素)\r\n",
        "    all_words = set(emission_counts.keys())\r\n",
        "    \r\n",
        "    # Get the number of (word, POS) tuples in the corpus 'y'\r\n",
        "    total = len(y)\r\n",
        "    # 利用zip組合prep及y，效果如下：\r\n",
        "    # test (guess_tag) test_word true_tag\r\n",
        "    #  ↑   ↑      ↑  ↑  ↑\r\n",
        "    # word  pos_final     y_tup\r\n",
        "    for word, y_tup in zip(prep, y): \r\n",
        "\r\n",
        "        # Split the (word, POS) string into a list of two items\r\n",
        "        y_tup_l = y_tup.split()\r\n",
        "        \r\n",
        "        # Verify that y_tup contain both word and POS\r\n",
        "        if len(y_tup_l) == 2:\r\n",
        "            \r\n",
        "            # 取出tag\r\n",
        "            true_label = y_tup_l[1]\r\n",
        "\r\n",
        "        else:\r\n",
        "            # If the y_tup didn't contain word and POS, go to next word\r\n",
        "            continue\r\n",
        "    \r\n",
        "        count_final = 0  #取emission count最大的\r\n",
        "        pos_final = ''   #取emission coint最大的所代表的tag\r\n",
        "        \r\n",
        "        # If the word is in the vocabulary...\r\n",
        "        if word in vocab:\r\n",
        "            for pos in states:\r\n",
        "\r\n",
        "            ### START CODE HERE (Replace instances of 'None' with your code) ###\r\n",
        "                        \r\n",
        "                # define the key as the tuple containing the POS and word\r\n",
        "                key = (pos,word)\r\n",
        "\r\n",
        "                # check if the (pos, word) key exists in the emission_counts dictionary\r\n",
        "                if key in emission_counts: # complete this line\r\n",
        "\r\n",
        "                # get the emission count of the (pos,word) tuple \r\n",
        "                    count = emission_counts[key]\r\n",
        "\r\n",
        "                    # keep track of the POS with the largest count\r\n",
        "                    if count>count_final: # complete this line\r\n",
        "\r\n",
        "                        # update the final count (largest count)\r\n",
        "                        count_final = count\r\n",
        "\r\n",
        "                        # update the final POS\r\n",
        "                        pos_final = pos\r\n",
        "\r\n",
        "            # If the final POS (with the largest count) matches the true POS:\r\n",
        "            if pos_final == true_label: # complete this line\r\n",
        "            \r\n",
        "                # Update the number of correct predictions\r\n",
        "                num_correct += 1\r\n",
        "            \r\n",
        "    ### END CODE HERE ###\r\n",
        "    accuracy = num_correct / total\r\n",
        "    \r\n",
        "    return accuracy\r\n",
        "\r\n",
        "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\r\n",
        "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LeUKFFGrtKo"
      },
      "source": [
        "優化HMM(with Viterbi)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnKpS5r9r8Fx"
      },
      "source": [
        "製作teansmition matrix(with smoothing)\r\n",
        "\r\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giSqyLv7rsoZ"
      },
      "source": [
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\r\n",
        "    ''' \r\n",
        "    Input: \r\n",
        "        alpha: number used for smoothing\r\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\r\n",
        "        transition_counts: transition count for the previous word and tag\r\n",
        "    Output:\r\n",
        "        A: matrix of dimension (num_tags,num_tags)\r\n",
        "    '''\r\n",
        "    # Get a sorted list of unique POS tags\r\n",
        "    all_tags = sorted(tag_counts.keys())\r\n",
        "    \r\n",
        "    # Count the number of unique POS tags\r\n",
        "    num_tags = len(all_tags)\r\n",
        "    \r\n",
        "    # Initialize the transition matrix 'A'\r\n",
        "    A = np.zeros((num_tags,num_tags))\r\n",
        "    \r\n",
        "    # Get the unique transition tuples (previous POS, current POS)\r\n",
        "    trans_keys = set(transition_counts.keys())\r\n",
        "    \r\n",
        "    ### START CODE HERE (Return instances of 'None' with your code) ### \r\n",
        "    \r\n",
        "    # Go through each row of the transition matrix A\r\n",
        "    for i in range(num_tags):\r\n",
        "        \r\n",
        "        # Go through each column of the transition matrix A\r\n",
        "        for j in range(num_tags):\r\n",
        "\r\n",
        "            # Initialize the count of the (prev POS, current POS) to zero\r\n",
        "            count = 0\r\n",
        "        \r\n",
        "            # Define the tuple (prev POS, current POS)\r\n",
        "            # Get the tag at position i and tag at position j (from the all_tags list)\r\n",
        "            key = (all_tags[i],all_tags[j])\r\n",
        "\r\n",
        "            # Check if the (prev POS, current POS) tuple \r\n",
        "            # exists in the transition counts dictionaory\r\n",
        "            if key in transition_counts: #complete this line\r\n",
        "                \r\n",
        "                # Get count from the transition_counts dictionary \r\n",
        "                # for the (prev POS, current POS) tuple\r\n",
        "                count = transition_counts[key]\r\n",
        "                \r\n",
        "            # Get the count of the previous tag (index position i) from tag_counts\r\n",
        "            count_prev_tag = tag_counts[all_tags[i]]\r\n",
        "            \r\n",
        "            # Apply smoothing using count of the tuple, alpha, \r\n",
        "            # count of previous tag, alpha, and number of total tags\r\n",
        "            A[i,j] = (count + alpha) / (count_prev_tag + alpha*num_tags)\r\n",
        "\r\n",
        "    ### END CODE HERE ###\r\n",
        "    \r\n",
        "    return A\r\n",
        "    \r\n",
        "alpha = 0.001\r\n",
        "for i in range(46):\r\n",
        "    tag_counts.pop(i,None)\r\n",
        "\r\n",
        "A = create_transition_matrix(alpha, tag_counts, transition_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8ZjQffAt7VJ"
      },
      "source": [
        "製作emission probabilities matrix\r\n",
        "\r\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxl7foBsuA8a"
      },
      "source": [
        "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\r\n",
        "    '''\r\n",
        "    Input: \r\n",
        "        alpha: tuning parameter used in smoothing \r\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\r\n",
        "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\r\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\r\n",
        "    Output:\r\n",
        "        B: a matrix of dimension (num_tags, len(vocab))\r\n",
        "    '''\r\n",
        "    \r\n",
        "    # get the number of POS tag\r\n",
        "    num_tags = len(tag_counts)\r\n",
        "    \r\n",
        "    # Get a list of all POS tags\r\n",
        "    all_tags = sorted(tag_counts.keys())\r\n",
        "    \r\n",
        "    # Get the total number of unique words in the vocabulary\r\n",
        "    num_words = len(vocab)\r\n",
        "    \r\n",
        "    # Initialize the emission matrix B with places for\r\n",
        "    # tags in the rows and words in the columns\r\n",
        "    B = np.zeros((num_tags, num_words))\r\n",
        "    \r\n",
        "    # Get a set of all (POS, word) tuples \r\n",
        "    # from the keys of the emission_counts dictionary\r\n",
        "    emis_keys = set(list(emission_counts.keys()))\r\n",
        "    \r\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\r\n",
        "    \r\n",
        "    # Go through each row (POS tags)\r\n",
        "    for i in range(num_tags): # complete this line\r\n",
        "        \r\n",
        "        # Go through each column (words)\r\n",
        "        for j in range(num_words): # complete this line\r\n",
        "\r\n",
        "            # Initialize the emission count for the (POS tag, word) to zero\r\n",
        "            count = 0\r\n",
        "                    \r\n",
        "            # Define the (POS tag, word) tuple for this row and column\r\n",
        "            key = (all_tags[i],vocab[j])\r\n",
        "            # check if the (POS tag, word) tuple exists as a key in emission counts\r\n",
        "            if key in emission_counts.keys(): # complete this line\r\n",
        "        \r\n",
        "                # Get the count of (POS tag, word) from the emission_counts d\r\n",
        "                count = emission_counts[key]\r\n",
        "                \r\n",
        "            # Get the count of the POS tag\r\n",
        "            count_tag = tag_counts[all_tags[i]]\r\n",
        "                \r\n",
        "            # Apply smoothing and store the smoothed value \r\n",
        "            # into the emission matrix B for this row and column\r\n",
        "            B[i,j] = (count + alpha) / (count_tag+ alpha*num_words)\r\n",
        "\r\n",
        "    ### END CODE HERE ###\r\n",
        "    return B\r\n",
        "\r\n",
        "for i in range(46):\r\n",
        "    tag_counts.pop(i,None)\r\n",
        "\r\n",
        "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcNF8riZudKe"
      },
      "source": [
        "Viterbi Algorithm\r\n",
        "\r\n",
        "\r\n",
        "* Initialization\r\n",
        "* Forward\r\n",
        "* Backward\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5qa73kquSTB"
      },
      "source": [
        "def initialize(states, tag_counts, A, B, corpus, vocab):\r\n",
        "    '''\r\n",
        "    Input: \r\n",
        "        states: a list of all possible parts-of-speech\r\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\r\n",
        "        A: Transition Matrix of dimension (num_tags, num_tags)\r\n",
        "        B: Emission Matrix of dimension (num_tags, len(vocab))\r\n",
        "        corpus: a sequence of words whose POS is to be identified in a list \r\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\r\n",
        "    Output:\r\n",
        "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\r\n",
        "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\r\n",
        "    '''\r\n",
        "    # Get the total number of unique POS tags\r\n",
        "    num_tags = len(tag_counts)\r\n",
        "    \r\n",
        "    # Initialize best_probs matrix \r\n",
        "    # POS tags in the rows, number of words in the corpus as the columns\r\n",
        "    best_probs = np.zeros((num_tags, len(corpus)))\r\n",
        "    \r\n",
        "    # Initialize best_paths matrix\r\n",
        "    # POS tags in the rows, number of words in the corpus as columns\r\n",
        "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\r\n",
        "    \r\n",
        "    # Define the start token\r\n",
        "    s_idx = states.index(\"--s--\")\r\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\r\n",
        "    \r\n",
        "    # Go through each of the POS tags\r\n",
        "    for i in range(num_tags): # complete this line\r\n",
        "        \r\n",
        "        # Handle the special case when the transition from start token to POS tag i is zero\r\n",
        "        if A[s_idx,i] == 0: # complete this line\r\n",
        "            \r\n",
        "            # Initialize best_probs at POS tag 'i', column 0, to negative infinity\r\n",
        "            best_probs[i,0] = float('-inf')\r\n",
        "        \r\n",
        "        # For all other cases when transition from start token to POS tag i is non-zero:\r\n",
        "        else:\r\n",
        "            \r\n",
        "            # Initialize best_probs at POS tag 'i', column 0\r\n",
        "            # Check the formula in the instructions above\r\n",
        "            best_probs[i,0] = math.log(A[s_idx,i]) + math.log(B[i,vocab[corpus[0]]] )\r\n",
        "                        \r\n",
        "    ### END CODE HERE ### \r\n",
        "    return best_probs, best_paths\r\n",
        "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZcPmbIyu1nQ"
      },
      "source": [
        "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\r\n",
        "    '''\r\n",
        "    Input: \r\n",
        "        A, B: The transiton and emission matrices respectively\r\n",
        "        test_corpus: a list containing a preprocessed corpus\r\n",
        "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\r\n",
        "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\r\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index \r\n",
        "    Output: \r\n",
        "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\r\n",
        "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\r\n",
        "    '''\r\n",
        "    # Get the number of unique POS tags (which is the num of rows in best_probs)\r\n",
        "    num_tags = best_probs.shape[0]\r\n",
        "    \r\n",
        "    # Go through every word in the corpus starting from word 1\r\n",
        "    # Recall that word 0 was initialized in `initialize()`\r\n",
        "    for i in range(1, len(test_corpus)): \r\n",
        "        \r\n",
        "        # Print number of words processed, every 5000 words\r\n",
        "        if i % 5000 == 0:\r\n",
        "            print(\"Words processed: {:>8}\".format(i))\r\n",
        "            \r\n",
        "        ### START CODE HERE (Replace instances of 'None' with your code EXCEPT the first 'best_path_i = None') ###\r\n",
        "        # For each unique POS tag that the current word can be\r\n",
        "        for j in range(num_tags): # complete this line\r\n",
        "            \r\n",
        "            # Initialize best_prob for word i to negative infinity\r\n",
        "            best_prob_i =  float(\"-inf\")\r\n",
        "            \r\n",
        "            # Initialize best_path for current word i to None\r\n",
        "            best_path_i = None\r\n",
        "\r\n",
        "            # For each POS tag that the previous word can be:\r\n",
        "            for k in range(num_tags): # complete this line\r\n",
        "            \r\n",
        "                # Calculate the probability = \r\n",
        "                # best probs of POS tag k, previous word i-1 + \r\n",
        "                # log(prob of transition from POS k to POS j) + \r\n",
        "                # log(prob that emission of POS j is word i)\r\n",
        "                prob = best_probs[k,i-1]+math.log(A[k,j]) +math.log(B[j,vocab[test_corpus[i]]])\r\n",
        "                # check if this path's probability is greater than\r\n",
        "                # the best probability up to and before this point\r\n",
        "                if prob > best_prob_i: # complete this line\r\n",
        "                    \r\n",
        "                    # Keep track of the best probability\r\n",
        "                    best_prob_i = prob\r\n",
        "                    \r\n",
        "                    # keep track of the POS tag of the previous word\r\n",
        "                    # that is part of the best path.  \r\n",
        "                    # Save the index (integer) associated with \r\n",
        "                    # that previous word's POS tag\r\n",
        "                    best_path_i = k\r\n",
        "\r\n",
        "            # Save the best probability for the \r\n",
        "            # given current word's POS tag\r\n",
        "            # and the position of the current word inside the corpus\r\n",
        "            best_probs[j,i] = best_prob_i\r\n",
        "            \r\n",
        "            # Save the unique integer ID of the previous POS tag\r\n",
        "            # into best_paths matrix, for the POS tag of the current word\r\n",
        "            # and the position of the current word inside the corpus.\r\n",
        "            best_paths[j,i] = best_path_i\r\n",
        "\r\n",
        "        ### END CODE HERE ###\r\n",
        "    return best_probs, best_paths\r\n",
        "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HgvP9XOu-I5"
      },
      "source": [
        "def viterbi_backward(best_probs, best_paths, corpus, states):\r\n",
        "    '''\r\n",
        "    This function returns the best path.\r\n",
        "    \r\n",
        "    '''\r\n",
        "    # Get the number of words in the corpus\r\n",
        "    # which is also the number of columns in best_probs, best_paths\r\n",
        "    m = best_paths.shape[1] \r\n",
        "    \r\n",
        "    # Initialize array z, same length as the corpus\r\n",
        "    z = [None] * m\r\n",
        "    \r\n",
        "    # Get the number of unique POS tags\r\n",
        "    num_tags = best_probs.shape[0]\r\n",
        "    \r\n",
        "    # Initialize the best probability for the last word\r\n",
        "    best_prob_for_last_word = float('-inf')\r\n",
        "    \r\n",
        "    # Initialize pred array, same length as corpus\r\n",
        "    pred = [None] * m\r\n",
        "    \r\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\r\n",
        "    ## Step 1 ##\r\n",
        "    \r\n",
        "    # Go through each POS tag for the last word (last column of best_probs)\r\n",
        "    # in order to find the row (POS tag integer ID) \r\n",
        "    # with highest probability for the last word\r\n",
        "    for k in range(num_tags): # complete this line\r\n",
        "\r\n",
        "        # If the probability of POS tag at row k \r\n",
        "        # is better than the previosly best probability for the last word:\r\n",
        "        if best_probs[k, m - 1]>best_prob_for_last_word: # complete this line\r\n",
        "            \r\n",
        "            # Store the new best probability for the last word\r\n",
        "            best_prob_for_last_word = best_probs[k, m - 1]\r\n",
        "    \r\n",
        "            # Store the unique integer ID of the POS tag\r\n",
        "            # which is also the row number in best_probs\r\n",
        "            z[m - 1]=k\r\n",
        "            \r\n",
        "    # Convert the last word's predicted POS tag\r\n",
        "    # from its unique integer ID into the string representation\r\n",
        "    # using the 'states' dictionary\r\n",
        "    # store this in the 'pred' array for the last word\r\n",
        "    pred[m - 1] = states[z[m - 1]]\r\n",
        "    \r\n",
        "    ## Step 2 ##\r\n",
        "    # Find the best POS tags by walking backward through the best_paths\r\n",
        "    # From the last word in the corpus to the 0th word in the corpus\r\n",
        "    for i in range(m-1, -1, -1): # complete this line\r\n",
        "        \r\n",
        "        # Retrieve the unique integer ID of\r\n",
        "        # the POS tag for the word at position 'i' in the corpus\r\n",
        "        pos_tag_for_word_i = z[i]\r\n",
        "        \r\n",
        "        # In best_paths, go to the row representing the POS tag of word i\r\n",
        "        # and the column representing the word's position in the corpus\r\n",
        "        # to retrieve the predicted POS for the word at position i-1 in the corpus\r\n",
        "        z[i - 1] = best_paths[pos_tag_for_word_i,i]\r\n",
        "        \r\n",
        "        # Get the previous word's POS tag in string form\r\n",
        "        # Use the 'states' dictionary, \r\n",
        "        # where the key is the unique integer ID of the POS tag,\r\n",
        "        # and the value is the string representation of that POS tag\r\n",
        "        pred[i - 1] = states[z[i - 1]]\r\n",
        "        \r\n",
        "     ### END CODE HERE ###\r\n",
        "    return pred\r\n",
        "pred = viterbi_backward(best_probs, best_paths, prep, states)\r\n",
        "m=len(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX4WL786vIyQ"
      },
      "source": [
        "def compute_accuracy(pred, y):\r\n",
        "    '''\r\n",
        "    Input: \r\n",
        "        pred: a list of the predicted parts-of-speech \r\n",
        "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\r\n",
        "    Output: \r\n",
        "        \r\n",
        "    '''\r\n",
        "    num_correct = 0\r\n",
        "    total = 0\r\n",
        "    \r\n",
        "    # Zip together the prediction and the labels\r\n",
        "    for prediction, y in zip(pred, y):\r\n",
        "        ### START CODE HERE (Replace instances of 'None' with your code) ###\r\n",
        "        # Split the label into the word and the POS tag\r\n",
        "        word_tag_tuple = y.split()\r\n",
        "        \r\n",
        "        # Check that there is actually a word and a tag\r\n",
        "        # no more and no less than 2 items\r\n",
        "        if len(word_tag_tuple)!=2: # complete this line\r\n",
        "            continue \r\n",
        "        # store the word and tag separately\r\n",
        "        word, tag = word_tag_tuple\r\n",
        "        # Check if the POS tag label matches the prediction\r\n",
        "        if prediction == tag: # complete this line\r\n",
        "            # count the number of times that the prediction\r\n",
        "            # and label match\r\n",
        "            num_correct += 1\r\n",
        "            \r\n",
        "        # keep track of the total number of examples (that have valid labels)\r\n",
        "        total += 1\r\n",
        "        \r\n",
        "        ### END CODE HERE ###\r\n",
        "    return (num_correct/total)\r\n",
        "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}