{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7d3Vp8We4TR"
      },
      "source": [
        "# 引入Libraries & 必要Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFspF6eodJjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a311bb00-0ea9-444a-e70b-5c957af1a1d9"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import numpy as np\r\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8klKt8sRfJUm"
      },
      "source": [
        "from scipy import linalg  #提供線代函式\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "\r\n",
        "def sigmoid(z):\r\n",
        "    # sigmoid function\r\n",
        "    return 1.0/(1.0+np.exp(-z))\r\n",
        "\r\n",
        "\r\n",
        "def get_idx(words, word2Ind):\r\n",
        "    idx = []\r\n",
        "    for word in words:\r\n",
        "        idx = idx + [word2Ind[word]]\r\n",
        "    return idx\r\n",
        "\r\n",
        "\r\n",
        "def pack_idx_with_frequency(context_words, word2Ind):\r\n",
        "    freq_dict = defaultdict(int)\r\n",
        "    for word in context_words:\r\n",
        "        freq_dict[word] += 1\r\n",
        "    idxs = get_idx(context_words, word2Ind)\r\n",
        "    packed = []\r\n",
        "    for i in range(len(idxs)):\r\n",
        "        idx = idxs[i]\r\n",
        "        freq = freq_dict[context_words[i]]\r\n",
        "        packed.append((idx, freq))\r\n",
        "    return packed\r\n",
        "\r\n",
        "\r\n",
        "def get_vectors(data, word2Ind, V, C):\r\n",
        "    i = C\r\n",
        "    while True:\r\n",
        "        y = np.zeros(V)\r\n",
        "        x = np.zeros(V)\r\n",
        "        center_word = data[i]\r\n",
        "        y[word2Ind[center_word]] = 1\r\n",
        "        context_words = data[(i - C):i] + data[(i+1):(i+C+1)]\r\n",
        "        num_ctx_words = len(context_words)\r\n",
        "        for idx, freq in pack_idx_with_frequency(context_words, word2Ind):\r\n",
        "            x[idx] = freq/num_ctx_words\r\n",
        "        yield x, y\r\n",
        "        i += 1\r\n",
        "        if i >= len(data):\r\n",
        "            print('i is being set to 0')\r\n",
        "            i = 0\r\n",
        "\r\n",
        "\r\n",
        "def get_batches(data, word2Ind, V, C, batch_size):\r\n",
        "    batch_x = []\r\n",
        "    batch_y = []\r\n",
        "    for x, y in get_vectors(data, word2Ind, V, C):\r\n",
        "        while len(batch_x) < batch_size:\r\n",
        "            batch_x.append(x)\r\n",
        "            batch_y.append(y)\r\n",
        "        else:\r\n",
        "            yield np.array(batch_x).T, np.array(batch_y).T\r\n",
        "            batch = []\r\n",
        "\r\n",
        "\r\n",
        "def compute_pca(data, n_components=2):\r\n",
        "    \"\"\"\r\n",
        "    Input: \r\n",
        "        data: of dimension (m,n) where each row corresponds to a word vector\r\n",
        "        n_components: Number of components you want to keep.\r\n",
        "    Output: \r\n",
        "        X_reduced: data transformed in 2 dims/columns + regenerated original data\r\n",
        "    pass in: data as 2D NumPy array\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    m, n = data.shape\r\n",
        "\r\n",
        "    ### START CODE HERE ###\r\n",
        "    # mean center the data\r\n",
        "    data -= data.mean(axis=0)\r\n",
        "    # calculate the covariance matrix\r\n",
        "    R = np.cov(data, rowvar=False)\r\n",
        "    # calculate eigenvectors & eigenvalues of the covariance matrix\r\n",
        "    # use 'eigh' rather than 'eig' since R is symmetric,\r\n",
        "    # the performance gain is substantial\r\n",
        "    evals, evecs = linalg.eigh(R)\r\n",
        "    # sort eigenvalue in decreasing order\r\n",
        "    # this returns the corresponding indices of evals and evecs\r\n",
        "    idx = np.argsort(evals)[::-1]\r\n",
        "\r\n",
        "    evecs = evecs[:, idx]\r\n",
        "    # sort eigenvectors according to same index\r\n",
        "    evals = evals[idx]\r\n",
        "    # select the first n eigenvectors (n is desired dimension\r\n",
        "    # of rescaled data array, or dims_rescaled_data)\r\n",
        "    evecs = evecs[:, :n_components]\r\n",
        "    ### END CODE HERE ###\r\n",
        "    return np.dot(evecs.T, data.T).T\r\n",
        "\r\n",
        "\r\n",
        "def get_dict(data):\r\n",
        "    \"\"\"\r\n",
        "    Input:\r\n",
        "        K: the number of negative samples\r\n",
        "        data: the data you want to pull from\r\n",
        "        indices: a list of word indices\r\n",
        "    Output:\r\n",
        "        word_dict: a dictionary with the weighted probabilities of each word\r\n",
        "        word2Ind: returns dictionary mapping the word to its index\r\n",
        "        Ind2Word: returns dictionary mapping the index to its word\r\n",
        "    \"\"\"\r\n",
        "    #\r\n",
        "#     words = nltk.word_tokenize(data)\r\n",
        "    words = sorted(list(set(data)))\r\n",
        "    n = len(words)\r\n",
        "    idx = 0\r\n",
        "    # return these correctly\r\n",
        "    word2Ind = {}\r\n",
        "    Ind2word = {}\r\n",
        "    for k in words:\r\n",
        "        word2Ind[k] = idx\r\n",
        "        Ind2word[idx] = k\r\n",
        "        idx += 1\r\n",
        "    return word2Ind, Ind2word\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9teDNRrmgyX4",
        "outputId": "a3130ba2-09f9-4b62-a939-4f85d858c425"
      },
      "source": [
        "# 上傳shakespeare.txt'\r\n",
        "import re                                   \r\n",
        "data = open('shakespeare.txt').read()                     #  讀取莎士比亞做corpus\r\n",
        "data = re.sub(r'[,!?;-]', '.',data)                      #  標點符號統一換成句號.\r\n",
        "data = nltk.word_tokenize(data)                          # 斷詞\r\n",
        "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    # 留下單詞及句號，並將單詞統一小寫\r\n",
        "print(\"Number of tokens:\", len(data),'\\n', data[:15])            "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tokens: 60933 \n",
            " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k00qxEgiTGe",
        "outputId": "e3bd5959-a504-457d-b6ea-d4604f4e5459"
      },
      "source": [
        "# 將vocabulary轉成陣列儲存並給予編號\r\n",
        "word2Ind, Ind2word = get_dict(data)\r\n",
        "V = len(word2Ind)\r\n",
        "print(\"Size of vocabulary: \", V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of vocabulary:  5772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQsoQSTSinLe"
      },
      "source": [
        "# 訓練模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akE_dCnLik-w"
      },
      "source": [
        "np.random.seed(1)\r\n",
        "# 初始化模型參數\r\n",
        "def initialize_model(N,V):\r\n",
        "\r\n",
        "    W1 = np.random.rand(N,V)\r\n",
        "    W2 = np.random.rand(V,N)\r\n",
        "    b1 = np.random.rand(N,1)\r\n",
        "    b2 = np.random.rand(V,1)     \r\n",
        "\r\n",
        "    return W1, W2, b1, b2\r\n",
        "\r\n",
        "def softmax(x):\r\n",
        "    \r\n",
        "    s = (np.exp(x) / np.sum(np.exp(x), axis=1)).T\r\n",
        "    \r\n",
        "    return s\r\n",
        "\r\n",
        "def forward_prop(x, W1, W2, b1, b2):\r\n",
        "    '''\r\n",
        "    Inputs: \r\n",
        "        x: average one hot vector for the context \r\n",
        "        W1, W2, b1, b2:  matrices and biases to be learned\r\n",
        "     Outputs: \r\n",
        "        z:  output score vector\r\n",
        "        h:  output of first hidden layer\r\n",
        "    '''\r\n",
        "   \r\n",
        "    h = np.dot(W1,x) + b1   \r\n",
        "    h = max(0,h)  # 使用ReLU 做激勵函數(若h>0則回傳h,若h<0則回傳0)\r\n",
        "    z = np.dot(W2, h) + b2\r\n",
        "\r\n",
        "    h = h.T\r\n",
        "    z = z.T\r\n",
        "\r\n",
        "\r\n",
        "    return z, h\r\n",
        "\r\n",
        "# Cost Function\r\n",
        "try:\r\n",
        "    from scipy.misc import logsumexp\r\n",
        "except ImportError:\r\n",
        "    from scipy.special import logsumexp\r\n",
        "def compute_cost(z, C, y, yhat, batch_size):\r\n",
        "    z_hat = logsumexp(z, axis=-1, keepdims=True)                      \r\n",
        "    cost = (-np.sum(y*np.log(yhat)) + np.sum(2.0*C*z_hat)) / batch_size\r\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRImvkszAUoh"
      },
      "source": [
        "def back_prop(x, z, y, h, W1, W2, b1, b2, batch_size, m):\r\n",
        "    '''\r\n",
        "    Inputs: \r\n",
        "        x:  average one hot vector for the context \r\n",
        "        z:  score vector\r\n",
        "        y:  target vector\r\n",
        "        h:  hidden vector (see eq. 1)\r\n",
        "        W1, W2, b1, b2:  matrices and biases  \r\n",
        "        batch_size: batch size \r\n",
        "        m:  number of context words\r\n",
        "     Outputs: \r\n",
        "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \r\n",
        "    '''\r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR OWN CODE) ###\r\n",
        "    ### (YOU WILL NEED TO ADD CODE IN ADDITION TO JUST REPLACING THE 'None' VALUES) ###\r\n",
        "    l1 = np.dot(W2.T, (z - y))\r\n",
        "    l1[l1<0] = 0.0\r\n",
        "    \r\n",
        "    grad_W2 = (1.0/(batch_size*m)) * np.dot((z - y), h)\r\n",
        "\r\n",
        "    grad_b2 = (1.0/(batch_size*m)) * np.sum((z - y), axis = 1, keepdims=True)\r\n",
        "    \r\n",
        "    grad_b1 = (1.0/(batch_size*m)) * np.sum(l1, axis = 1, keepdims=True)\r\n",
        "\r\n",
        "    grad_W1 = (1.0/(batch_size*m)) * np.dot(l1, x.T)\r\n",
        "    ### END CODE HERE ###\r\n",
        "\r\n",
        "    return grad_W1, grad_W2, grad_b1, grad_b2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-DGmISCAgpV"
      },
      "source": [
        "def gradient_descent(data, word2Ind, C, N, V, num_iters, alpha=0.03):\r\n",
        "    \r\n",
        "    '''\r\n",
        "    This is the gradient_descent function\r\n",
        "    \r\n",
        "      Inputs: \r\n",
        "        data:      text\r\n",
        "        word2Ind:  words to Indices\r\n",
        "        C:         context window\r\n",
        "        N:         dimension of hidden vector  \r\n",
        "        V:         dimension of vocabulary \r\n",
        "        num_iters: number of iterations  \r\n",
        "     Outputs: \r\n",
        "        W1, W2, b1, b2:  updated matrices and biases   \r\n",
        "\r\n",
        "    '''\r\n",
        "    W1, W2, b1, b2 = initialize_model(N,V) #W1=(N,V) and W2=(V,N)\r\n",
        "    m = (2*C)\r\n",
        "    batch_size = 128\r\n",
        "    iters = 0\r\n",
        "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\r\n",
        "        z, h = forward_prop(x, W1, W2, b1, b2)\r\n",
        "        yhat = softmax(z)\r\n",
        "        cost = compute_cost(z, C, y, yhat, batch_size)\r\n",
        "        print('iters', iters + 1 , '   cost',cost)\r\n",
        "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size, m)\r\n",
        "        W1 = W1 - alpha * grad_W1 \r\n",
        "        W2 = W2 - alpha * grad_W2\r\n",
        "        b1 = b1 - alpha * grad_b1 \r\n",
        "        b2 = b2 - alpha * grad_b2\r\n",
        "        iters += 1 \r\n",
        "        if iters == num_iters: \r\n",
        "            break\r\n",
        "        if iters % 100 == 0:\r\n",
        "            alpha *= 0.66\r\n",
        "    \r\n",
        "        ### END CODE HERE ###\r\n",
        "\r\n",
        "    return W1, W2, b1, b2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up3awXctAxQg"
      },
      "source": [
        "from matplotlib import pyplot\r\n",
        "%config InlineBackend.figure_format = 'svg'\r\n",
        "words = ['king', 'queen','lord','man', 'woman','dog','horse',\r\n",
        "         'rich','happy','sad']\r\n",
        "\r\n",
        "embs = (W1.T + W2)/2.0\r\n",
        " \r\n",
        "# given a list of words and the embeddings, it returns a matrix with all the embeddings\r\n",
        "idx = [word2Ind[word] for word in words]\r\n",
        "X = embs[idx, :]\r\n",
        "print(X.shape, idx)  # X.shape:  Number of words of dimension N each"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAHMUVmFAzW4"
      },
      "source": [
        "result= compute_pca(X, 2)\r\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\r\n",
        "for i, word in enumerate(words):\r\n",
        "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\r\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7lV0VOzA23X"
      },
      "source": [
        "result= compute_pca(X, 4)\r\n",
        "pyplot.scatter(result[:, 3], result[:, 1])\r\n",
        "for i, word in enumerate(words):\r\n",
        "    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\r\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}